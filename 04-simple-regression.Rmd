# Simple regression (fixed effects)

## Regression in general

Regression is the explicit modelling of a parametric association between an outcome and an exposure.

One such parametric association might be the following:

$$\text{outcome} = 3 + 2 \times \text{exposure}$$

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(data.table)
library(ggplot2)

data <- data.frame(x=-10:10)
data$y <- 3 + 2*data$x

q <- ggplot(data,aes(x=x,y=y))
q <- q + geom_vline(xintercept=0,colour="red")
q <- q + geom_hline(yintercept=0,colour="red")
q <- q + geom_line()
q <- q + scale_x_continuous("Exposure")
q <- q + scale_y_continuous("Outcome")
q
```

Depending on the type of outcome, different types of regression will need to be used.

For all regressions, the exposure can be:

- Continuous
- Binary (0 or 1)
- Categorical (0, 1, 2, ...)
- Count data

Regressions can both:

- Perform hypothesis testing (same as the previous tests we have learned about)
- Estimate numerically the effect size of the association between outcome and exposure (new!)

## Linear regression

In the most basic form, we have:

$$\text{outcome} = \beta_0 + \beta_1 \times \text{exposure} + \text{error}$$

Where we aim to estimate values for $\beta_0$ and $\beta_1$.

For example, if we run an ice cream shop:

$$\text{number of ice creams sold} = 5 + 3 \times \text{temperature} + \text{error}$$

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(data.table)
library(ggplot2)

data <- data.frame(x=-10:40)
data$line <- 5 + 3*data$x
data$y <- data$line + rnorm(nrow(data),sd=10)

q <- ggplot(data,aes(x=x))
q <- q + geom_vline(xintercept=0,colour="red")
q <- q + geom_hline(yintercept=0,colour="red")
q <- q + geom_point(mapping=aes(y=y))
q <- q + geom_line(mapping=aes(y=line))
q <- q + scale_x_continuous("Temperature")
q <- q + scale_y_continuous("Number of ice creams sold")
q
```

If today's temperature is 30C, we can expect our shop to sell $5 + 3 \times 30 = 95$ ice creams. Because $\beta_1$ ($=3$) was not zero, we have a significant association between temperature and number of ice creams sold.

Another example, if we work as a midwife:

$$\text{Child's birthweight} = 3 + 0 \times \text{temperature at day of delivery} + \text{error}$$

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(data.table)
library(ggplot2)

data <- data.frame(x=-10:40)
data$line <- 3 + 0*data$x
data$y <- data$line + rnorm(nrow(data),sd=0.5)

q <- ggplot(data,aes(x=x))
q <- q + geom_vline(xintercept=0,colour="red")
q <- q + geom_hline(yintercept=0,colour="red")
q <- q + geom_point(mapping=aes(y=y))
q <- q + geom_line(mapping=aes(y=line))
q <- q + scale_x_continuous("Temperature")
q <- q + scale_y_continuous("Child's birthweight")
q
```

If today's temperature is 30C, we can expect that children born today will be (on average) $3 + 0 \times 30 = 3$ kg. If tomorrow's temperature is 10C, we can expect that children born today will be (on average) $3 + 0 \times 10 = 3$ kg. Because $\beta_1$ was zero, we do not have a significant association between temperature and birthweight.

### Aim/Outcome/Exposure/Parametric/Dependencies

**Aim**: Hypothesis testing and estimating the effect size of the association between outcome and exposure

**Outcome**: *Continuous variable*

**Exposure**: Continuous, Binary, Categorical, Count variable

**Parametric assumptions**: Residuals are distributed as a Normal distribution

**Dependencies**: None (all observations independent)

\newpage

### Example 1

$\rightarrow$ Testing if average birth weight (continuous outcome) is associated with parents' income (continuous exposure)

$$\text{birth weight} = \beta_0 + \beta_1 \times \text{parent's income} + \text{error}$$

$$\text{H}_0: \beta_1 = 0$$
$$\text{H}_1: \beta_1 \ne 0$$

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(data.table)
library(ggplot2)

data <- data.frame(x=10:100)
data$line <- 3 + 0.01*data$x
data$y <- data$line + rnorm(nrow(data),sd=0.5)

q <- ggplot(data,aes(x=x))
q <- q + geom_vline(xintercept=0,colour="red")
q <- q + geom_hline(yintercept=0,colour="red")
q <- q + geom_point(mapping=aes(y=y))
q <- q + geom_line(mapping=aes(y=line))
q <- q + scale_x_continuous("Parent's income ($1000s)")
q <- q + scale_y_continuous("Child's birthweight")
q
```

\newpage

### Example 2

$\rightarrow$ Testing if average birth weight (continuous outcome) is associated with child's sex (binary exposure)

$$\text{birth weight} = \beta_0 + \beta_1 \times \text{is boy} + \text{error}$$

$$\text{H}_0: \beta_1 = 0$$
$$\text{H}_1: \beta_1 \ne 0$$

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(data.table)
library(ggplot2)

data <- data.frame(x=c(rep(0,100),rep(1,100)))
data$line <- 3 + 1*data$x
data$y <- data$line + rnorm(nrow(data),sd=0.5)

q <- ggplot(data,aes(x=x))
q <- q + geom_vline(xintercept=0,colour="red")
q <- q + geom_hline(yintercept=0,colour="red")
q <- q + geom_point(mapping=aes(y=y))
q <- q + geom_line(mapping=aes(y=line))
q <- q + scale_x_continuous("Child's sex (is male)")
q <- q + scale_y_continuous("Child's birthweight")
q
```

\newpage

### Example 3

$\rightarrow$ Testing if average BMI levels (continuous outcome) differ across Scandinavia (categorical exposure)

$$\text{bmi} = \beta_0 + \beta_1 \times \text{is Norway} + \beta_2 \times \text{is Sweden} + \text{error}$$

$$\text{H}_0: \beta_1 = \beta_2 = 0$$
$$\text{H}_1: \beta_1 \ne 0 \text{ and/or } \beta_2 \ne 0$$

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(data.table)
library(ggplot2)

data <- data.frame(x=c(rep(0,100),rep(1,100),rep(2,100)))
data$line <- 18
data$line[data$x==1] <- 23
data$line[data$x==2] <- 55
data$y <- data$line + rnorm(nrow(data),sd=4)

q <- ggplot(data,aes(x=x))
q <- q + geom_vline(xintercept=0,colour="red")
q <- q + geom_hline(yintercept=0,colour="red")
q <- q + geom_point(mapping=aes(y=y))
q <- q + geom_line(mapping=aes(y=line))
q <- q + scale_x_continuous("Country",breaks=c(0,1,2),labels=c("Denmark","Norway","Sweden"))
q <- q + scale_y_continuous("BMI")
q
```

### Example 4

$\rightarrow$
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
$H_0:$
\hfill \break
\hfill \break
\hfill \break
$H_1:$

### Example 5

$\rightarrow$
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
$H_0:$
\hfill \break
\hfill \break
\hfill \break
$H_1:$

\newpage

## Similarities between t-tests, ANOVA, and linear regression

### Example 1

**Two-sample unpaired t-test**:

$\rightarrow$ Testing if average birth weight (continuous outcome) is different in female children versus male children

$$\text{H}_0: \mu_{\text{boys}} = \mu_{\text{girls}}$$
$$\text{H}_1: \mu_{\text{boys}} \ne \mu_{\text{girls}}$$

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(data.table)
library(ggplot2)

data1 <- data.frame(x=rnorm(1000,mean=2,sd=1), group="Female", stringsAsFactors = F)
data2 <- data.frame(x=rnorm(1000,mean=3,sd=1), group="Male", stringsAsFactors = F)
data <- rbind(data1,data2)
q <- ggplot(data,aes(x=x,fill=group))
q <- q + geom_histogram(alpha=0.5,colour="black",pos="dodge")
q <- q + scale_x_continuous("Weight (Kg)")
q <- q + scale_y_continuous("Number of babies")
q <- q + scale_fill_brewer("",palette="Set1")
q
```

\newpage

**ANOVA**:

$\rightarrow$ Testing if average birth weight (continuous outcome) is different in female children versus male children

$$\text{H}_0: \mu_{\text{boys}} = \mu_{\text{girls}}$$
$$\text{H}_1: \mu_{\text{boys}} \ne \mu_{\text{girls}}$$

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(data.table)
library(ggplot2)

data1 <- data.frame(x=rnorm(1000,mean=2,sd=1), group="Female", stringsAsFactors = F)
data2 <- data.frame(x=rnorm(1000,mean=3,sd=1), group="Male", stringsAsFactors = F)
data <- rbind(data1,data2)
q <- ggplot(data,aes(x=x,fill=group))
q <- q + geom_histogram(alpha=0.5,colour="black",pos="dodge")
q <- q + scale_x_continuous("Weight (Kg)")
q <- q + scale_y_continuous("Number of babies")
q <- q + scale_fill_brewer("",palette="Set1")
q
```

\newpage

**Linear regression**:

$\rightarrow$ Testing if the effect of child's sex on average birth weight (continuous outcome) is different than zero

$$\text{birth weight} = \beta_0 + \beta_1 \times \text{is boy} + \text{error}$$
$$\text{H}_0: \beta_1 = 0$$
$$\text{H}_1: \beta_1 \ne 0$$

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(data.table)
library(ggplot2)

data <- data.frame(x=c(rep(0,100),rep(1,100)))
data$line <- 2
data$line[data$x==1] <- 3
data$y <- data$line + rnorm(nrow(data),sd=1)

q <- ggplot(data,aes(x=x))
q <- q + geom_vline(xintercept=0,colour="red")
q <- q + geom_hline(yintercept=0,colour="red")
q <- q + geom_point(mapping=aes(y=y))
q <- q + geom_line(mapping=aes(y=line))
q <- q + scale_x_continuous("Sex",breaks=c(0,1),labels=c("Female","Male"))
q <- q + scale_y_continuous("Kg")
q
```

**Conclusion**:

- Two-sample unpaired t-tests are ANOVAs with only two groups
- Two-sample unpaired t-tests are linear regressions with a binary (0/1) exposure
- ANOVA is a linear regression with a categorical exposure

\newpage

### Example 2

**Two-sample unpaired t-test**:

$\rightarrow$ Testing if average number of hours sleep (continuous outcome) is different in adults who are parents versus those who are childless

$$\text{H}_0: \mu_{\text{parents}} = \mu_{\text{childless}}$$
$$\text{H}_1: \mu_{\text{parents}} \ne \mu_{\text{childless}}$$

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(data.table)
library(ggplot2)

data1 <- data.frame(x=rnorm(1000,mean=8,sd=0.5), group="Childless", stringsAsFactors = F)
data2 <- data.frame(x=rnorm(1000,mean=5,sd=0.5), group="Parents", stringsAsFactors = F)
data <- rbind(data1,data2)
q <- ggplot(data,aes(x=x,fill=group))
q <- q + geom_histogram(alpha=0.5,colour="black",pos="dodge")
q <- q + scale_x_continuous("Hours sleep")
q <- q + scale_y_continuous("Number of people")
q <- q + scale_fill_brewer("",palette="Set1")
q
```

\newpage

**ANOVA**:

$\rightarrow$ Testing if average number of hours sleep (continuous outcome) is different in adults who are parents versus those who are childless

$$\text{H}_0: \mu_{\text{parents}} = \mu_{\text{childless}}$$
$$\text{H}_1: \mu_{\text{parents}} \ne \mu_{\text{childless}}$$

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(data.table)
library(ggplot2)

data1 <- data.frame(x=rnorm(1000,mean=8,sd=0.5), group="Childless", stringsAsFactors = F)
data2 <- data.frame(x=rnorm(1000,mean=5,sd=0.5), group="Parents", stringsAsFactors = F)
data <- rbind(data1,data2)
q <- ggplot(data,aes(x=x,fill=group))
q <- q + geom_histogram(alpha=0.5,colour="black",pos="dodge")
q <- q + scale_x_continuous("Hours sleep")
q <- q + scale_y_continuous("Number of people")
q <- q + scale_fill_brewer("",palette="Set1")
q
```

\newpage

**Linear regression**:

$\rightarrow$ Testing if the effect of being a parent on average number of hours sleep (continuous outcome) is different than zero

$$\text{birth weight} = \beta_0 + \beta_1 \times \text{is parent} + \text{error}$$
$$\text{H}_0: \beta_1 = 0$$
$$\text{H}_1: \beta_1 \ne 0$$

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(data.table)
library(ggplot2)

data <- data.frame(x=c(rep(0,100),rep(1,100)))
data$line <- 8
data$line[data$x==1] <- 5
data$y <- data$line + rnorm(nrow(data),sd=0.5)

q <- ggplot(data,aes(x=x))
q <- q + geom_vline(xintercept=0,colour="red")
q <- q + geom_hline(yintercept=0,colour="red")
q <- q + geom_point(mapping=aes(y=y))
q <- q + geom_line(mapping=aes(y=line))
q <- q + scale_x_continuous("Parental status",breaks=c(0,1),labels=c("Childless","Parents"))
q <- q + scale_y_continuous("Hours of sleep")
q
```

**Conclusion**:

- Two-sample unpaired t-tests are ANOVAs with only two groups
- Two-sample unpaired t-tests are linear regressions with a binary (0/1) exposure
- ANOVA is a linear regression with a categorical exposure

\newpage

### Example 3

**Two-sample unpaired t-test**:

$\rightarrow$
\hfill \break
\hfill \break
\hfill \break
$H_0:$
\hfill \break
\hfill \break
\hfill \break
$H_1:$
\hfill \break
\hfill \break
\hfill \break

**ANOVA**:

$\rightarrow$
\hfill \break
\hfill \break
\hfill \break
$H_0:$
\hfill \break
\hfill \break
\hfill \break
$H_1:$
\hfill \break
\hfill \break
\hfill \break

**Linear regression**:

$\rightarrow$
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
$H_0:$
\hfill \break
\hfill \break
\hfill \break
$H_1:$
\hfill \break
\hfill \break
\hfill \break

\newpage

## Similarities between ANOVA and linear regression

### Example 1

**ANOVA**:

$\rightarrow$ Testing if average birth weight (continuous outcome) differs between Scandinavian countries

$$\text{H}_0: \mu_{\text{Norway}} = \mu_{\text{Denmark}} = \mu_{\text{Sweden}}$$
$$\text{H}_1: \mu_{\text{Norway}} \ne \mu_{\text{Denmark}} \text{ and/or } \mu_{\text{Norway}} \ne \mu_{\text{Sweden}}  \text{ and/or } \mu_{\text{Denmark}} \ne \mu_{\text{Sweden}}$$

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(data.table)
library(ggplot2)

data1 <- data.frame(x=rnorm(1000,mean=2,sd=0.5), group="Denmark", stringsAsFactors = F)
data2 <- data.frame(x=rnorm(1000,mean=3,sd=0.5), group="Norway", stringsAsFactors = F)
data3 <- data.frame(x=rnorm(1000,mean=4,sd=0.5), group="Sweden", stringsAsFactors = F)
data <- rbind(data1,data2,data3)
q <- ggplot(data,aes(x=x,fill=group))
q <- q + geom_histogram(alpha=0.5,colour="black",pos="dodge")
q <- q + scale_x_continuous("Weight (Kg)")
q <- q + scale_y_continuous("Number of babies")
q <- q + scale_fill_brewer("",palette="Set1")
q
```

\newpage

**Linear regression**:

$\rightarrow$ Testing if the effect of country on average birth weight (continuous outcome) is different than zero

$$\text{birth weight} = \beta_0 + \beta_1 \times \text{is Norway} + \beta_2 \times \text{is Denmark} + \text{error}$$
$$\text{H}_0: \beta_1 = \beta_2 = 0$$
$$\text{H}_1: \beta_1 \ne 0 \text{ and/or } \beta_2 \ne 0$$

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(data.table)
library(ggplot2)

data <- data.frame(x=c(rep(0,100),rep(1,100),rep(2,100)))
data$line <- 2
data$line[data$x==1] <- 3
data$line[data$x==2] <- 4
data$y <- data$line + rnorm(nrow(data),sd=0.5)

q <- ggplot(data,aes(x=x))
q <- q + geom_vline(xintercept=0,colour="red")
q <- q + geom_hline(yintercept=0,colour="red")
q <- q + geom_point(mapping=aes(y=y))
q <- q + geom_line(mapping=aes(y=line))
q <- q + scale_x_continuous("Country",breaks=c(0,1,2),labels=c("Denmark","Norway","Sweden"))
q <- q + scale_y_continuous("BMI")
q
```

**Conclusion**:

- ANOVA is a linear regression with a categorical exposure

\newpage

### Example 2

**ANOVA**:

$\rightarrow$
\hfill \break
\hfill \break
\hfill \break
$H_0:$
\hfill \break
\hfill \break
\hfill \break
$H_1:$
\hfill \break
\hfill \break
\hfill \break

**Linear regression**:

$\rightarrow$
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
$H_0:$
\hfill \break
\hfill \break
\hfill \break
$H_1:$
\hfill \break
\hfill \break
\hfill \break

## Logistic regression models

Logistic regression is essentially the same as linear regression, but it is used when:

- You have a binary (0/1) outcome
- You are doing a case-control study [case control studies can ONLY be analysed using logistic regression]

### Aim/Outcome/Exposure/Parametric/Dependencies

**Aim**: Hypothesis testing and estimating the effect size of the association between outcome and exposure

**Outcome**: *Binary variable*

**Exposure**: Continuous, Binary, Categorical, Count variable

**Parametric assumptions**: No

**Dependencies**: None (all observations independent)

### Example 1

$\rightarrow$ Testing if percentage of women (binary outcome) differ across the bydels of Oslo (categorical exposure)

$$\text{log}\left(\frac{\text{Pr(Is woman)}}{\text{Pr(Is man)}}\right) = \beta_0 + \beta_1 \times \text{bydel}_1 + \beta_2 \times \text{bydel}_2 + \beta_3 \times \text{bydel}_3 + \text{error}$$

$$\text{H}_0: \beta_1 = \beta_2 = \beta_3 = 0$$
$$\text{H}_1: \beta_1 \ne 0 \text{and/or} \beta_2 \ne 0 \text{and/or} \beta_3 \ne 0$$

### Example 2

$\rightarrow$ Testing if risk of unemployment (binary outcome) is associated with parents' income (continuous exposure)

$$\text{log}\left(\frac{\text{Pr(Is unemployed)}}{\text{Pr(Is employed)}}\right) = \beta_0 + \beta_1 \times \text{parent's income} + \text{error}$$

$$\text{H}_0: \beta_1 = 0$$
$$\text{H}_1: \beta_1 \ne 0$$

### Example 3

$\rightarrow$ Testing if risk of smoking (binary outcome) is associated with parents' smoking status (binary exposure)

$$\text{log}\left(\frac{\text{Pr(Is smoker)}}{\text{Pr(Is not smoker)}}\right) = \beta_0 + \beta_1 \times \text{parent's are smokers} + \text{error}$$

$$\text{H}_0: \beta_1 = 0$$
$$\text{H}_1: \beta_1 \ne 0$$

### Example 4

$\rightarrow$
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
$H_0:$
\hfill \break
\hfill \break
\hfill \break
$H_1:$

### Example 5

$\rightarrow$
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
$H_0:$
\hfill \break
\hfill \break
\hfill \break
$H_1:$

\newpage

## Poisson/negative-binomial regression models

Poisson/negative-binomial regression is essentially the same as linear regression, but it is used when:

- You have a count outcome

Negative-binomial regression is a more flexible version of poisson regression. Poisson regression requires that the residual variation (after fitting the model) is equal to the expected mean. This is quite often not the case. Negative-binomial regression fits the variation and the mean separately, removing this problem. It is therefore recommended that you always use a negative-binomial regression instead of a poisson regression. The only exception is if you encounter statistical errors with the negative-binomial regression (i.e. it won't converge/run), then a poisson regression is your only option.

### Aim/Outcome/Exposure/Parametric/Dependencies

**Aim**: Hypothesis testing and estimating the effect size of the association between outcome and exposure

**Outcome**: *Count variable*

**Exposure**: Continuous, Binary, Categorical, Count variable

**Parametric assumptions for Poisson**: Mean equals variable

**Parametric assumptions for negative-binomial**: No

**Dependencies**: None (all observations independent)

### Example 1

$\rightarrow$ Testing if average number of influenza cases (count outcome) is different between 2000-2009 and 2010-2015 (binary exposure) in Norway

$$\text{yearly number of influenza cases} = \beta_0 + \beta_1 \times \text{is 2010 to 2015} + \text{error}$$

$$\text{H}_0: \beta_1 = 0$$
$$\text{H}_1: \beta_1 \ne 0$$

### Example 2

$\rightarrow$
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
$H_0:$
\hfill \break
\hfill \break
\hfill \break
$H_1:$

\newpage

## Cox regression models

Cox regression models should be used when your outcome is "time-to-event".

The most common example of this is when you are following a cohort of people over time, trying to observe an (e.g. sickness, death, response). Your outcome is "length of time until person X gets disease Y". However, a number of your participants stop responding at some point, so you only know "person X was healthy up until 200 days, when we lost contact".Thus person X's outcome has been censored at day 200.

### Aim/Outcome/Exposure/Parametric/Dependencies

**Aim**: Hypothesis testing and estimating the effect size of the association between outcome and exposure

**Outcome**: *Censored variable* (time-to-event)

**Exposure**: Continuous, Binary, Categorical, Count variable

**Parametric assumptions**: Proportional hazards

**Dependencies**: None (all observations independent)

### Example 1

$\rightarrow$ Testing if time-to-death (outcome) is associated with having a hospital-acquired-infection after hip surgery (binary exposure)

$$\lambda(t | X_i) = \lambda_0(t) \times \text{exp}(\beta_1 \times \text{had HAI})$$

Where $\lambda(t | X_i)$ is the hazard rate of dying at time $t$ for subject $i$.

$$\text{H}_0: \beta_1 = 0$$
$$\text{H}_1: \beta_1 \ne 0$$

### Example 2

$\rightarrow$
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
$H_0:$
\hfill \break
\hfill \break
\hfill \break
$H_1:$
