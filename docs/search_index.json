[
["index.html", "Which Stats Method Should I Use? Chapter 1 Syllabus", " Which Stats Method Should I Use? Richard White 2017-09-12 Chapter 1 Syllabus Instructor: Richard White [richard.white@fhi.no] Time: 09:00 - 11:45, 18th September 2017 Location: Main auditorium, L8, Lindern Campus, Folkehelseinstittutet, Oslo Language: English Format and Procedures 09:00 - 10:00: Lecture 1 10:00 - 10:10: Break 10:10 - 11:10: Lecture 2 10:10 - 10:15: Break 11:15 - 11:45: Examples from FHI Description This course will provide a basic overview of general statistical methodology that can be useful in the areas of infectious diseases, environmental medicine, and labwork. By the end of this course, students will be able to identify appropriate statistical methods for a variety of circumstances. This course will not teach students how to implement these statistical methods, as there is not sufficient time. The aim of this course is to enable the student to identify which methods are required for their study, allowing the student to identify their needs for subsequent methods courses, self-learning, or external help. You should register for this course if you are one of the following: Have experience with applying statistical methods, but are sometimes confused or uncertain as to whether or not you have selected the correct method. Do not have experience with applying statistical methods, and would like to get an overview over which methods are applicable for your projects so that you can then undertake further studies in these areas. Lecture 1 Identifying continuous, categorical, count, and censored variables Identifying exposure and outcome variables Identifying when t-tests (paired and unpaired) should be used Identifying when non-parametric t-test equivalents should be used Identifying when ANOVA should be used Identifying when linear regression should be used Identifying the similarities between t-tests, ANOVA, and regression Identifying when logistic regression models should be used Identifying when Poisson/negative binomial and cox regression models should be used Identifying when chi-squared/fisher’s exact test should be used Lecture 2 Identifying when data does not have any dependencies (i.e. all observations are independent of each other) versus when data has complicated dependencies (i.e. longitudinal data, matched data, multiple cohorts) Identifying when mixed effects regression models should be used Identifying when conditional logistic regression models should be used (TBD) Understanding the different imputation methods used when lab data is below the limit of detection (LOD) (TBD) Understanding the best practices for data files and project folders Prerequisites To participate in this course it is recommended that you have some experience with either research or data. Additional information For the last 30 minutes of the course we will be going through examples of analyses performed at FHI and identifying which statistical methods are appropriate. If you would like your analysis to be featured/included in this section, please send an email to richard.white@fhi.no briefly describing your problem. "],
["reference.html", "Chapter 2 Reference", " Chapter 2 Reference "],
["variable-types.html", "Chapter 3 Variable Types 3.1 Continuous Variables 3.2 Binary Variables 3.3 Categorical Variable 3.4 Censored Variables 3.5 Count Variables 3.6 Independent Versus Dependent Variables", " Chapter 3 Variable Types 3.1 Continuous Variables A variable is continuous there is a meaningful “distance” between values. For example: Temperature Weight Height BMI Blood pressure 3.2 Binary Variables A variable is binary if it can only hold two values. For example: 0 or 1 True or false Male or female Sick or healthy Born in Norway vs Born outside of Norway 3.3 Categorical Variable A variable is categorical if there is no meaningful “distance” between values. For example: Sick or healthy Born in Norway vs Born outside of Norway Cancer stage (I, II, III, or IV) BMI category (underweight, normal, or overweight) 3.4 Censored Variables Censored variables are a subset of continuous variables. They are artificially cutoff (“censored”) at some point. For example: Height – if everyone over 175cm is recorded as “175+” Age – if everyone under 10 years old is recorded as “&lt;=10” Time alive since receiving illness diagnosis if there is loss to followup (i.e. we know that the person has lived at least 4 years before we lost track of them) 3.5 Count Variables Count variables are a subset of continuous variables. They can only have integer values (e.g. 0, 1, 2, 3). For example: Number of cars that use the parking lot in a day Number of influenza patients who use the hospital every day Number of tuberculosis patients who are screened every year 3.6 Independent Versus Dependent Variables An independent variable is often called an exposure or predictor variable. In an experiment, this variable is manipulated by the researcher. A dependent variable is often called the outcome. In research, we generally want to see if (the following all mean the same thing): The dependent variable is dependent on the independent variable The predictor variable predicts the outcome. The exposure affects the outcome For ease of understanding, we will use the terms “outcome” and “exposure” for the rest of this course. "],
["simple-hypothesis-testing-chi-squared-t-tests-and-anova.html", "Chapter 4 Simple Hypothesis Testing: Chi-Squared, T-tests, and ANOVA 4.1 Hypothesis Testing 4.2 Which Method To Use? 4.3 Chi-Squared Test 4.4 One Sample T-Test 4.5 Two sample T-Tests 4.6 Two-sample Paired T-Test 4.7 Two-sample Unpaired T-Test 4.8 ANOVA", " Chapter 4 Simple Hypothesis Testing: Chi-Squared, T-tests, and ANOVA 4.1 Hypothesis Testing In science, we are interested in testing hypotheses. Statistics allows us to formally test our hypotheses. In statistical testing we have a null hypothesis (\\(\\text{H}_0\\)) and an alternative hypothesis (\\(\\text{H}_1\\)). We assume the null hypothesis is true and try to find the probability of what we have observed (or something more extreme). If our observations are very unlikely (assuming the null hypothesis is true) then we reject the null hypothesis in favor of the alternative hypothesis. For example: \\[\\text{H}_0: \\text{It is summer}\\] \\[\\text{H}_1: \\text{It is not summer}\\] Our observed data for today is a maximum temperature of -20C. Assuming it is summer, how likely is it that today’s maximum temperature will be -20C? Not very likely! We therefore reject \\(\\text{H}_0\\) (“it is summer”) in favor of \\(\\text{H}_1\\) (“it is not summer”). That is, we conclude that it is not summer today. 4.2 Which Method To Use? Deciding on the appropriate statistical method is (in principle) fairly easy. You just look at the: Aim (hypothesis testing or estimation of effect size?) Outcome type (continuous, binary, categorical, censored, count) Exposure (type) Parametric assumptions Dependencies in the data and we then (essentially) use a flowchart. 4.3 Chi-Squared Test A Chi-Squared test is used to test if two categorical variables are associated with each other. 4.3.1 Aim/Outcome/Exposure/Parametric/Dependencies Aim: Hypothesis testing (testing if two categorical variables are associated with each other.) Outcome: Categorical variable Exposure: Categorical variable Parametric assumptions: No Dependencies: None (all observations independent) 4.3.2 Examples Testing if people’s country of origin (Norway/Not Norway) is associated with tuberculosis status (never had TB/has had TB) Testing if people’s region of origin (Europe/North America/South America/Other) is associated with marital status (Single/Married/Divorced) Testing if county of residence (Oslo, Akershus, etc) is associated with post-surgery infection status (No infection/mild infection/deep infection) 4.4 One Sample T-Test A one sample t-test tests if the mean of a continuous variable differs from a specified value (generally zero) \\[\\text{H}_0: \\mu = 180\\] \\[\\text{H}_1: \\mu \\ne 180\\] Or rephrased: \\[\\text{H}_0: \\text{The average height of men is equal to 180cm}\\] \\[\\text{H}_1: \\text{The average height of men is not equal to 180cm}\\] 4.4.1 Aim/Outcome/Exposure/Parametric/Dependencies Aim: Hypothesis testing (test if the mean of a continuous variable differs from a specified value) Outcome: Continuous variable Exposure: Does not exist Parametric assumptions: Outcome is distributed as a Normal distribution Dependencies: None (all observations independent) 4.4.2 Example 1 \\(\\rightarrow\\) Testing if the average BMI of Norwegians is equal to 23 \\[H_0: \\mu_{\\text{bmi}} = 23\\] \\[H_1: \\mu_{\\text{bmi}} \\ne 23\\] 4.4.3 Example 2 \\(\\rightarrow\\) Testing if the average pH of tap water is equal to 7 \\[H_0: \\mu_{\\text{pH}} = 7\\] \\[H_1: \\mu_{\\text{pH}} \\ne 7\\] 4.4.4 Example 3 \\(\\rightarrow\\) \\(H_0:\\) \\(H_1:\\) 4.4.5 Example 4 \\(\\rightarrow\\) \\(H_0:\\) \\(H_1:\\) 4.5 Two sample T-Tests A t-test tests if the mean of a continuous variable differs between two groups. There are two kinds of two-sample t-tests: paired and unpaired. 4.6 Two-sample Paired T-Test A paired t-test is a special case where we have \\(N\\) participants, and each participant has two observations (generally “before experiment” and “after experiment”). We want to test if the mean of outcome variable differs between “after” and “before”. For example, in a weight-loss experiment, we have \\(N\\) participants and we want to see if the average “after weight” is different from the average “before weight”. This is done by subtracting the outcome from one group (“before weight”) from the outcome in the other group (“after weight”) for each person (“difference in weight”), and then performing a one-sample t-test to see if the mean of this variable is different from zero. \\[\\text{H}_0: \\mu_{\\text{after}-\\text{before}} = 0\\] \\[\\text{H}_1: \\mu_{\\text{after}-\\text{before}} \\ne 0\\] 4.6.1 Aim/Outcome/Exposure/Parametric/Dependencies Special preprocessing of data: for each participant subtract the “before” observation from the “after” observation Aim: Hypothesis testing (test if the mean of a continuous variable measured twice for each participant differs between “before” and “after”) Outcome: (“after weight” minus “before weight”) continuous variable Exposure: \\(\\text{group}_\\text{after}\\) vs \\(\\text{group}_\\text{before}\\) Parametric assumptions: Outcome is distributed as a Normal distribution Dependencies: Paired data 4.6.2 Example 1 \\(\\rightarrow\\) Testing if there is a difference in blood pressure before and after treatment (measured on the same person) \\[\\text{H}_0: \\mu_{\\text{BP after}-\\text{BP before}} = 0\\] \\[\\text{H}_1: \\mu_{\\text{BP after}-\\text{BP before}} \\ne 0\\] 4.6.3 Example 2 \\(\\rightarrow\\) Testing if there is a difference in mouse DNA damage before and after irradiating (measured on the same mouse) \\[\\text{H}_0: \\mu_{\\text{DNA damage after}-\\text{DNA damage before}} = 0\\] \\[\\text{H}_1: \\mu_{\\text{DNA damage after}-\\text{DNA damage before}} \\ne 0\\] 4.6.4 Example 3 \\(\\rightarrow\\) \\(H_0:\\) \\(H_1:\\) 4.6.5 Example 4 \\(\\rightarrow\\) \\(H_0:\\) \\(H_1:\\) 4.6.6 Non-Parametric Equivalent Wilcoxon signed-rank test. This should be used when the Normality assumption fails. 4.7 Two-sample Unpaired T-Test An unpaired t-test is where we have two independent groups of \\(N_1\\) and \\(N_2\\) participants, and we want to test if the mean of the outcome variable differs between \\(\\text{group}_1\\) and \\(\\text{group}_2\\). \\[\\text{H}_0: \\mu_0 = \\mu_1\\] \\[\\text{H}_1: \\mu_0 \\ne \\mu_1\\] Or rephrased: \\[\\text{H}_0: \\text{The average height of men is equal to the average height of women}\\] \\[\\text{H}_1: \\text{The average height of men is not equal to the average height of women}\\] 4.7.1 Aim/Outcome/Exposure/Parametric/Dependencies Aim: Hypothesis testing (test if the mean of a continuous variable differs between \\(\\text{group}_1\\) and \\(\\text{group}_2\\)) Outcome: continuous variable Exposure: \\(\\text{group}_1\\) vs \\(\\text{group}_2\\) Parametric assumptions: Outcomes for each group are distributed as a Normal distribution Dependencies: None (all observations independent) 4.7.2 Example 1 \\(\\rightarrow\\) Testing if the average blood pressure in people who didn’t receive treatment is different from people who did receive treatment \\[\\text{H}_0: \\mu_{\\text{BP treatment}} = \\mu_{\\text{BP no treatment}}\\] \\[\\text{H}_1: \\mu_{\\text{BP treatment}} \\ne \\mu_{\\text{BP no treatment}}\\] 4.7.3 Example 2 \\(\\rightarrow\\) Testing if the average DNA damage in mice that weren’t irradiated is different from mice that were irradiated \\[\\text{H}_0: \\mu_{\\text{DNA radiation}} = \\mu_{\\text{DNA no radiation}}\\] \\[\\text{H}_1: \\mu_{\\text{DNA radiation}} \\ne \\mu_{\\text{DNA no radiation}}\\] 4.7.4 Example 3 \\(\\rightarrow\\) \\(H_0:\\) \\(H_1:\\) 4.7.5 Example 4 \\(\\rightarrow\\) \\(H_0:\\) \\(H_1:\\) 4.7.6 Non-parametric equivalent Mann–Whitney U test (also called the Mann–Whitney–Wilcoxon (MWW), Wilcoxon rank-sum test, or Wilcoxon–Mann–Whitney test). This should be used when the Normality assumption fails. 4.8 ANOVA ANOVA is an extension of the two-sample unpaired t-test. We have X independent groups of participants, and we want to test if the mean of the outcome variable differs between groups. 4.8.1 Aim/Outcome/Exposure/Parametric/Dependencies Aim: Hypothesis testing (test if the mean of a continuous variable differs between some groups) Outcome: continuous variable Exposure: \\(\\text{group}_1\\) vs \\(\\text{group}_2\\) (vs \\(\\text{group}_3\\) …) Parametric assumptions: Outcomes for each group are distributed as a Normal distribution Dependencies: None (all observations independent) 4.8.2 Example 1 \\(\\rightarrow\\) Testing if average BMI levels differ across Scandinavia \\[\\text{H}_0: \\mu_{\\text{Norway}} = \\mu_{\\text{Denmark}} = \\mu_{\\text{Sweden}}\\] \\[\\text{H}_1: \\mu_{\\text{Norway}} \\ne \\mu_{\\text{Denmark}} \\text{ and/or } \\mu_{\\text{Norway}} \\ne \\mu_{\\text{Sweden}} \\text{ and/or } \\mu_{\\text{Denmark}} \\ne \\mu_{\\text{Sweden}}\\] 4.8.3 Example 2 \\(\\rightarrow\\) Testing if average water pH levels differ between East and West Oslo \\[\\text{H}_0: \\mu_{\\text{East Oslo}} = \\mu_{\\text{West Oslo}}\\] \\[\\text{H}_1: \\mu_{\\text{East Oslo}} \\ne \\mu_{\\text{West Oslo}}\\] 4.8.4 Example 3 \\(\\rightarrow\\) \\(H_0:\\) \\(H_1:\\) 4.8.5 Example 4 \\(\\rightarrow\\) \\(H_0:\\) \\(H_1:\\) 4.8.6 Non-parametric equivalent Kruskal–Wallis test "],
["simple-regression-fixed-effects.html", "Chapter 5 Simple regression (fixed effects) 5.1 Regression in general 5.2 Linear regression 5.3 Similarities between t-tests, ANOVA, and linear regression 5.4 Similarities between ANOVA and linear regression 5.5 Logistic regression models 5.6 Poisson/negative-binomial regression models 5.7 Cox regression models should be used", " Chapter 5 Simple regression (fixed effects) 5.1 Regression in general Regression is the explicit modelling of a parametric association between an outcome and an exposure. One such parametric association might be the following: \\[\\text{outcome} = 3 + 2 \\times \\text{exposure}\\] Depending on the type of outcome, different types of regression will need to be used. For all regressions, the exposure can be: Continuous Binary (0 or 1) Categorical (0, 1, 2, …) Count data Regressions can both: Perform hypothesis testing (same as the previous tests we have learned about) Estimate numerically the effect size of the association between outcome and exposure (new!) 5.2 Linear regression In the most basic form, we have: \\[\\text{outcome} = \\beta_0 + \\beta_1 \\times \\text{exposure} + \\text{error}\\] Where we aim to estimate values for \\(\\beta_0\\) and \\(\\beta_1\\). For example, if we run an ice cream shop: \\[\\text{number of ice creams sold} = 5 + 3 \\times \\text{temperature} + \\text{error}\\] If today’s temperature is 30C, we can expect our shop to sell \\(5 + 3 \\times 30 = 95\\) ice creams. Because \\(\\beta_1\\) (\\(=3\\)) was not zero, we have a significant association between temperature and number of ice creams sold. Another example, if we work as a midwife: \\[\\text{Child&#39;s birthweight} = 3 + 0 \\times \\text{temperature at day of delivery} + \\text{error}\\] If today’s temperature is 30C, we can expect that children born today will be (on average) \\(3 + 0 \\times 30 = 3\\) kg. If tomorrow’s temperature is 10C, we can expect that children born today will be (on average) \\(3 + 0 \\times 10 = 3\\) kg. Because \\(\\beta_1\\) was zero, we do not have a significant association between temperature and birthweight. 5.2.1 Aim/Outcome/Exposure/Parametric/Dependencies Aim: Hypothesis testing and estimating the effect size of the association between outcome and exposure Outcome: Continuous variable Exposure: Continuous, Binary, Categorical, Count variable Parametric assumptions: Residuals are distributed as a Normal distribution Dependencies: None (all observations independent) 5.2.2 Example 1 \\(\\rightarrow\\) Testing if average birth weight (continuous outcome) is associated with parents’ income (continuous exposure) \\[\\text{birth weight} = \\beta_0 + \\beta_1 \\times \\text{parent&#39;s income} + \\text{error}\\] \\[\\text{H}_0: \\beta_1 = 0\\] \\[\\text{H}_1: \\beta_1 \\ne 0\\] 5.2.3 Example 2 \\(\\rightarrow\\) Testing if average birth weight (continuous outcome) is associated with child’s sex (binary exposure) \\[\\text{birth weight} = \\beta_0 + \\beta_1 \\times \\text{is boy} + \\text{error}\\] \\[\\text{H}_0: \\beta_1 = 0\\] \\[\\text{H}_1: \\beta_1 \\ne 0\\] 5.2.4 Example 3 \\(\\rightarrow\\) Testing if average BMI levels (continuous outcome) differ across Scandinavia (categorical exposure) \\[\\text{bmi} = \\beta_0 + \\beta_1 \\times \\text{is Norway} + \\beta_2 \\times \\text{is Sweden} + \\text{error}\\] \\[\\text{H}_0: \\beta_1 = \\beta_2 = 0\\] \\[\\text{H}_1: \\beta_1 \\ne 0 \\text{ and/or } \\beta_2 \\ne 0\\] 5.2.5 Example 4 \\(\\rightarrow\\) \\(H_0:\\) \\(H_1:\\) 5.2.6 Example 5 \\(\\rightarrow\\) \\(H_0:\\) \\(H_1:\\) 5.3 Similarities between t-tests, ANOVA, and linear regression 5.3.1 Example 1 Two-sample unpaired t-test: \\(\\rightarrow\\) Testing if average birth weight (continuous outcome) is different in female children versus male children \\[\\text{H}_0: \\mu_{\\text{boys}} = \\mu_{\\text{girls}}\\] \\[\\text{H}_1: \\mu_{\\text{boys}} \\ne \\mu_{\\text{girls}}\\] ANOVA: \\(\\rightarrow\\) Testing if average birth weight (continuous outcome) is different in female children versus male children \\[\\text{H}_0: \\mu_{\\text{boys}} = \\mu_{\\text{girls}}\\] \\[\\text{H}_1: \\mu_{\\text{boys}} \\ne \\mu_{\\text{girls}}\\] Linear regression: \\(\\rightarrow\\) Testing if the effect of child’s sex on average birth weight (continuous outcome) is different than zero \\[\\text{birth weight} = \\beta_0 + \\beta_1 \\times \\text{is boy} + \\text{error}\\] \\[\\text{H}_0: \\beta_1 = 0\\] \\[\\text{H}_1: \\beta_1 \\ne 0\\] Conclusion: Two-sample unpaired t-tests are ANOVAs with only two groups Two-sample unpaired t-tests are linear regressions with a binary (0/1) exposure ANOVA is a linear regression with a categorical exposure 5.3.2 Example 2 Two-sample unpaired t-test: \\(\\rightarrow\\) Testing if average number of hours sleep (continuous outcome) is different in adults who are parents versus those who are childless \\[\\text{H}_0: \\mu_{\\text{parents}} = \\mu_{\\text{childless}}\\] \\[\\text{H}_1: \\mu_{\\text{parents}} \\ne \\mu_{\\text{childless}}\\] ANOVA: \\(\\rightarrow\\) Testing if average number of hours sleep (continuous outcome) is different in adults who are parents versus those who are childless \\[\\text{H}_0: \\mu_{\\text{parents}} = \\mu_{\\text{childless}}\\] \\[\\text{H}_1: \\mu_{\\text{parents}} \\ne \\mu_{\\text{childless}}\\] Linear regression: \\(\\rightarrow\\) Testing if the effect of being a parent on average number of hours sleep (continuous outcome) is different than zero \\[\\text{birth weight} = \\beta_0 + \\beta_1 \\times \\text{is parent} + \\text{error}\\] \\[\\text{H}_0: \\beta_1 = 0\\] \\[\\text{H}_1: \\beta_1 \\ne 0\\] Conclusion: Two-sample unpaired t-tests are ANOVAs with only two groups Two-sample unpaired t-tests are linear regressions with a binary (0/1) exposure ANOVA is a linear regression with a categorical exposure 5.3.3 Example 3 Two-sample unpaired t-test: \\(\\rightarrow\\) \\(H_0:\\) \\(H_1:\\) ANOVA: \\(\\rightarrow\\) \\(H_0:\\) \\(H_1:\\) Linear regression: \\(\\rightarrow\\) \\(H_0:\\) \\(H_1:\\) 5.4 Similarities between ANOVA and linear regression 5.4.1 Example 1 ANOVA: \\(\\rightarrow\\) Testing if average birth weight (continuous outcome) differs between Scandinavian countries \\[\\text{H}_0: \\mu_{\\text{Norway}} = \\mu_{\\text{Denmark}} = \\mu_{\\text{Sweden}}\\] \\[\\text{H}_1: \\mu_{\\text{Norway}} \\ne \\mu_{\\text{Denmark}} \\text{ and/or } \\mu_{\\text{Norway}} \\ne \\mu_{\\text{Sweden}} \\text{ and/or } \\mu_{\\text{Denmark}} \\ne \\mu_{\\text{Sweden}}\\] Linear regression: \\(\\rightarrow\\) Testing if the effect of country on average birth weight (continuous outcome) is different than zero \\[\\text{birth weight} = \\beta_0 + \\beta_1 \\times \\text{is Norway} + \\beta_2 \\times \\text{is Denmark} + \\text{error}\\] \\[\\text{H}_0: \\beta_1 = \\beta_2 = 0\\] \\[\\text{H}_1: \\beta_1 \\ne 0 \\text{ and/or } \\beta_2 \\ne 0\\] Conclusion: ANOVA is a linear regression with a categorical exposure 5.4.2 Example 2 ANOVA: \\(\\rightarrow\\) \\(H_0:\\) \\(H_1:\\) Linear regression: \\(\\rightarrow\\) \\(H_0:\\) \\(H_1:\\) 5.5 Logistic regression models Logistic regression is essentially the same as linear regression, but it is used when: You have a binary (0/1) outcome You are doing a case-control study [case control studies can ONLY be analysed using logistic regression] 5.5.1 Aim/Outcome/Exposure/Parametric/Dependencies Aim: Hypothesis testing and estimating the effect size of the association between outcome and exposure Outcome: Binary variable Exposure: Continuous, Binary, Categorical, Count variable Parametric assumptions: No Dependencies: None (all observations independent) 5.5.2 Example 1 \\(\\rightarrow\\) Testing if percentage of women (binary outcome) differ across the bydels of Oslo (categorical exposure) \\[\\text{log}\\left(\\frac{\\text{Pr(Is woman)}}{\\text{Pr(Is man)}}\\right) = \\beta_0 + \\beta_1 \\times \\text{bydel}_1 + \\beta_2 \\times \\text{bydel}_2 + \\beta_3 \\times \\text{bydel}_3 + \\text{error}\\] \\[\\text{H}_0: \\beta_1 = \\beta_2 = \\beta_3 = 0\\] \\[\\text{H}_1: \\beta_1 \\ne 0 \\text{and/or} \\beta_2 \\ne 0 \\text{and/or} \\beta_3 \\ne 0\\] 5.5.3 Example 2 \\(\\rightarrow\\) Testing if risk of unemployment (binary outcome) is associated with parents’ income (continuous exposure) \\[\\text{log}\\left(\\frac{\\text{Pr(Is unemployed)}}{\\text{Pr(Is employed)}}\\right) = \\beta_0 + \\beta_1 \\times \\text{parent&#39;s income} + \\text{error}\\] \\[\\text{H}_0: \\beta_1 = 0\\] \\[\\text{H}_1: \\beta_1 \\ne 0\\] 5.5.4 Example 3 \\(\\rightarrow\\) Testing if risk of smoking (binary outcome) is associated with parents’ smoking status (binary exposure) \\[\\text{log}\\left(\\frac{\\text{Pr(Is smoker)}}{\\text{Pr(Is not smoker)}}\\right) = \\beta_0 + \\beta_1 \\times \\text{parent&#39;s are smokers} + \\text{error}\\] \\[\\text{H}_0: \\beta_1 = 0\\] \\[\\text{H}_1: \\beta_1 \\ne 0\\] 5.5.5 Example 4 \\(\\rightarrow\\) \\(H_0:\\) \\(H_1:\\) 5.5.6 Example 5 \\(\\rightarrow\\) \\(H_0:\\) \\(H_1:\\) 5.6 Poisson/negative-binomial regression models Poisson/negative-binomial regression is essentially the same as linear regression, but it is used when: You have a count outcome Negative-binomial regression is a more flexible version of poisson regression. Poisson regression requires that the residual variation (after fitting the model) is equal to the expected mean. This is quite often not the case. Negative-binomial regression fits the variation and the mean separately, removing this problem. It is therefore recommended that you always use a negative-binomial regression instead of a poisson regression. The only exception is if you encounter statistical errors with the negative-binomial regression (i.e. it won’t converge/run), then a poisson regression is your only option. 5.6.1 Aim/Outcome/Exposure/Parametric/Dependencies Aim: Hypothesis testing and estimating the effect size of the association between outcome and exposure Outcome: Count variable Exposure: Continuous, Binary, Categorical, Count variable Parametric assumptions for Poisson: Mean equals variable Parametric assumptions for negative-binomial: No Dependencies: None (all observations independent) 5.6.2 Example 1 \\(\\rightarrow\\) Testing if average number of influenza cases (count outcome) is different between 2000-2009 and 2010-2015 (binary exposure) in Norway \\[\\text{yearly number of influenza cases} = \\beta_0 + \\beta_1 \\times \\text{is 2010 to 2015} + \\text{error}\\] \\[\\text{H}_0: \\beta_1 = 0\\] \\[\\text{H}_1: \\beta_1 \\ne 0\\] 5.6.3 Example 2 \\(\\rightarrow\\) \\(H_0:\\) \\(H_1:\\) 5.7 Cox regression models should be used Cox regression models should be used when your outcome is “time-to-event”. The most common example of this is when you are following a cohort of people over time, trying to observe an (e.g. sickness, death, response). Your outcome is “length of time until person X gets disease Y”. However, a number of your participants stop responding at some point, so you only know “person X was healthy up until 200 days, when we lost contact”.Thus person X’s outcome has been censored at day 200. 5.7.1 Aim/Outcome/Exposure/Parametric/Dependencies Aim: Hypothesis testing and estimating the effect size of the association between outcome and exposure Outcome: Censored variable (time-to-event) Exposure: Continuous, Binary, Categorical, Count variable Parametric assumptions: Proportional hazards Dependencies: None (all observations independent) 5.7.2 Example 1 \\(\\rightarrow\\) Testing if time-to-death (outcome) is associated with having a hospital-acquired-infection after hip surgery (binary exposure) \\[\\lambda(t | X_i) = \\lambda_0(t) \\times \\text{exp}(\\beta_1 \\times \\text{had HAI})\\] Where \\(\\lambda(t | X_i)\\) is the hazard rate of dying at time \\(t\\) for subject \\(i\\). \\[\\text{H}_0: \\beta_1 = 0\\] \\[\\text{H}_1: \\beta_1 \\ne 0\\] 5.7.3 Example 2 \\(\\rightarrow\\) \\(H_0:\\) \\(H_1:\\) "],
["complicated-regression.html", "Chapter 6 Complicated regression 6.1 Dependencies in your data 6.2 Analysing data with dependencies 6.3 (TBD) Understanding the best practices for data files and project folders", " Chapter 6 Complicated regression 6.1 Dependencies in your data 6.1.1 What is independent data Broadly, having knowledge about one observation should not give you knowledge about other observations in your dataset. For example, if we flip a coin ten times, knowing the result of the first coin toss (heads) will not give us knowledge about the subsequent 9 coin tosses. 6.1.2 What is data with dependencies In reality, most data have dependencies, so we will focus on some of the most important kinds that will severely impact your analyses if you do not identify them. 6.1.3 Repeated measures/longitudinal data If you have a dataset with repeated measures (e.g. some people in your cohort have more than one observation), then the repeated observations on each person cause dependencies in the data. That is, if a person has their weight measured five times, then just by knowing their first weight you can have a good guess at what their subsequent weights will be. 6.1.4 Matched data Inside case-control studies, for each case a control (or multiple cases) can be selected to have similar attributes. For example, for each case, a control can be selected with a similar age. These controls have been “matched” to a case, and have introduced dependencies into the data. 6.1.5 Grouped/clustered data Repeated measures data is a type of clustered data, where each person is their own cluster. Matched data is also a type of clustered data, where each group of matched controls-to-a-case is a cluster. There can be many other kinds of clusters, for example: Data sampled from multiple hospitals could have the hospital as the cluster variable Data sampled from multiple countries could have the country as the cluster variable Data sampled from multiple counties/municipalities could have the country/municipality as the cluster variable If it is a study of children, and multiple children from each mother are included, then the mother could be the cluster variable 6.2 Analysing data with dependencies 6.2.1 Mixed effects regression Mixed effects regression is an extension of the simple regression models (fixed effects) that we learned about in the previous chapter. You can have: Mixed effects linear regression Mixed effects logistic regression Mixed effects negative-binomial regression Mixed effects models are models that have both fixed and random effects. “Effects” is the term used to describe the estimated impact a variable has on the outcome. For example, “the effect of smoking on the risk of lung cancer”. To determine if an effect is fixed or random, we introduce the concept of pooling data (i.e. sharing strength). Let us assume we have sampled 100 people per city for 9 cities, and 2 people for 1 city, and we want to estimate the average height in each city, we can do one of the following three options: Within each city, take the average height of the 100 (or 2) sampled people (zero pooling) Take the average height of 1000 sampled people and say each city is the same height (complete pooling) Estimate how much the average height varies between the cities. If the average height doesn’t vary much, give estimates close to #2. If the average height varies a lot, give estimates close to #1 (partial pooling) Zero pooling tends to work well when you have a limited amount of effects you want to estimate, each with a good sample size (i.e. you don’t need to borrow strength from other data points). Partial pooling tends to work well when you have a large number of effects you want to estimate, each with a small sample size (i.e. you need to borrow strength from other data points). Fixed effects are estimated with zero pooling. Random effects are estimated with partial pooling. With data that has a large number of clusters, the clustering need to be accounted for in the regression model. This is done by introducing a variable that uniquely identifies each cluster, and allows the cluster effect to be estimated (e.g. in Oslo people are 1% more likely to die than in the rest of Norway). If there are a large number of clusters with a small amount of people in each cluster, then the cluster variable should be estimated using partial pooling (i.e. random effects). In summary: Mixed effects regression should be used for grouped/clustered/matched data (and subsequently repeated measures/longitudinal data). 6.2.2 Conditional logistic regression Conditional logistic regression can also be used for matched data with a binary outcome, however, it is less flexible than mixed effects regression. 6.3 (TBD) Understanding the best practices for data files and project folders "],
["examples-1.html", "Chapter 7 Examples 7.1 Poisons Information Center 7.2 Norwegian Water Pipes", " Chapter 7 Examples 7.1 Poisons Information Center img Scenario: Approximately 40 000 calls per year The frequency of calls regarding different drugs/plants etc varies greatly, from a couple per year to thousands per year, so most probably one method will not cover everything. The registration of data is done while on the phone, and we know there are mistakes Only the field Kommentar has free text Question: Is the number of calls regarding women aged 15-19 who have been exposed to paracetamol rising or falling over the years. Aim: Outcome: Exposure: Parametric assumptions: Dependencies: 7.2 Norwegian Water Pipes Scenario: 146 waterworks in 19 counties in Norway Each waterwork uses pipes to deliver water to households Information on each waterwork: Length of pipes made out of asbestos (in meters) Length of pipes made out of iron/steel (in meters) Length of pipes made out of PVC (in meters) Length of pipes made out of PE/PEH (in meters) Length of pipes made out of other (in meters) Length of pipes made out of unknown (in meters) Length of pipes installed before 1910 (in meters) Length of pipes installed in 1910-1940 (in meters) Length of pipes installed in 1941-1970 (in meters) Length of pipes installed in 1971-2000 (in meters) Length of pipes installed after 2000 (in meters) Length of pipes installed during an unknown period (in meters) Each year, some new pipes are laid to extend the network Each year, some pipes are replaced Interruption in water delivery is estimated in hours per calendar year Data is only for 2015 Question: Is there an association between “interuption in water delivery” and “type of pipe material” and “pipe installation period” Aim: Outcome: Exposure: Parametric assumptions: Dependencies: "],
["solutions.html", "Chapter 8 Solutions 8.1 Poisons Information Center 8.2 Norwegian Water Pipes", " Chapter 8 Solutions 8.1 Poisons Information Center Scenario: Approximately 40 000 calls per year The frequency of calls regarding different drugs/plants etc varies greatly, from a couple per year to thousands per year, so most probably one method will not cover everything. The registration of data is done while on the phone, and we know there are mistakes Only the field Kommentar has free text Question: Is the number of calls regarding women aged 15-19 who have been exposed to paracetamol rising or falling over the years. Aim: Hypothesis testing (maybe estimation of yearly effect) Outcome: Count data (number of calls regarding women aged 15-19 who have been exposed to paracetamol) Exposure: Continuous (year) Parametric assumptions: None Dependencies: None Appropriate Method: Negative-binomial regression Example STATA code: nbreg number_of_calls year 8.2 Norwegian Water Pipes Scenario: 146 waterworks in 19 counties in Norway Each waterwork uses pipes to deliver water to households Information on each waterwork: Length of pipes made out of asbestos (in meters) Length of pipes made out of iron/steel (in meters) Length of pipes made out of PVC (in meters) Length of pipes made out of PE/PEH (in meters) Length of pipes made out of other (in meters) Length of pipes made out of unknown (in meters) Length of pipes installed before 1910 (in meters) Length of pipes installed in 1910-1940 (in meters) Length of pipes installed in 1941-1970 (in meters) Length of pipes installed in 1971-2000 (in meters) Length of pipes installed after 2000 (in meters) Length of pipes installed during an unknown period (in meters) Each year, some new pipes are laid to extend the network Each year, some pipes are replaced Interruption in water delivery is estimated in hours per calendar year Data is only for 2015 Question: Is there an association between “interuption in water delivery” and “type of pipe material” and “pipe installation period” Aim: Hypothesis testing Outcome: Count (hours interruption in water delivery) Exposure: 12 continuous variables Parametric assumptions: None Dependencies: No Appropriate Method: Negative-binomial regression Example STATA code: nbreg hoursInterrupted pipesAsbestos pipesIronSteel pipesPVC ... pipes1910 ... "],
["references.html", "References", " References "]
]
