# Linear Regression vs ANOVA

## Summary

Many ANOVA computations can be performed using linear regression models, with nested/hierarchical problems requiring mixed effects regression models  [@gelman_analysis_2005].

## Empirical evidence

First we create some data, shown in Figure \@ref(fig:anova-data)

```{r, anova-data, fig.cap='Three diffferent groups (x=0, 1, 2)', out.width='80%', fig.asp=.75, fig.align='center'}
library(ggplot2)

set.seed(4)
x <- rep(0:2,100)
y <- (x+1)/7 + rnorm(length(x))
data <- data.frame(y=y,x=x)

q <- ggplot(data,aes(x=x,y=y,group=x))
q <- q + geom_boxplot()
print(q)
```

Then we establish the linear regression model

$$
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \epsilon_i
$$
where $i=1,\dots\,n$ and $\epsilon_i \sim N(0,\sigma^2)$

```{r}
fit <- lm(y ~ factor(x), data)
summary(fit)
```

and we can see that the F-test corresponding to

$$
H_0: \beta_1 = \beta_2 = 0
$$
$$
H_1: \text{Not } H_0
$$
has a p-value of 0.01203.

We then perform a one-way ANOVA assuming equal variances, with:
$$
H_0: \bar{y}_{\text{x=0}} = \bar{y}_{\text{x=1}} = \bar{y}_{\text{x=2}}
$$
$$
H_1: \text{Not } H_0
$$

```{r}
oneway.test(y~x,data = data,var.equal = TRUE)
```

and we again see that the p-value is 0.01203.

However, when running a one-way ANOVA assuming unequal variances

```{r}
oneway.test(y~x,data = data,var.equal = FALSE)
```

We see that the p-value is 0.01187, which is not the same as the linear regression.

## Statistical proof
The following proof was taken from [@hardy_difference_2012]

Suppose your data set consists of a set $(x_i,y_i)$ for $i=1,\ldots,n$ and you want to look at the dependence of $y$ on $x$.

Suppose you find the values $\hat\beta_0$ and $\hat\beta_1$ of $\beta_0$ and $\beta_1$ that minimize the residual sum of squares
$$
\sum_{i=1}^n (y_i - (\beta_0+\beta_1 x_i))^2.
$$
Then you take $\hat y = \hat\beta_0+ \hat\beta_1 x$ to be the predicted $y$-value for any (not necessarily already observed) $x$-value.  That's linear regression.

Now consider decomposing the total sum of squares
$$
\sum_{i=1}^n (y_i - \bar y)^2 \text{ where }\bar y = \frac{y_1+\cdots+y_n}{n}
$$
with $n-1$ degrees of freedom, into "explained" and "unexplained" parts:
$$
\underbrace{\sum_{i=1}^n ((\hat\beta_0+\hat\beta_1 x_i) - \bar y)^2}_{\text{explained}}\  +\  \underbrace{\sum_{i=1}^n (y_i - (\hat\beta_0+\hat\beta_1 x_i))^2}_{\text{unexplained}}.
$$
with $1$ and $n-2$ degrees of freedom, respectively.  That's analysis of variance, and one then considers things like F-statistics
$$
F = \frac{\sum_{i=1}^n ((\hat\beta_0+\hat\beta_1 x_i) - \bar y)^2/1}{\sum_{i=1}^n (y_i - (\hat\beta_0+\hat\beta_1 x_i))^2/(n-2)}.
$$
_This_ F-statistic tests the null hypothesis $\beta_1=0$, which is the same as the traditional ANOVA:
$$
F = \frac{\text{Between SS}/\text{df}}{\text{Within SS}/\text{df}}.
$$

One often first encounters the term "analysis of variance" when the predictor is categorical, so that you're fitting the model
$$
y = \beta_0 + \beta_i
$$
where $i$ identifies which category is the value of the predictor.  If there are $k$ categories, you'd get $k-1$ degrees of freedom in the numerator in the F-statistic, and usually $n-k$ degrees of freedom in the denominator.  But the distinction between regression and analysis of variance is still the same for this kind of model.

